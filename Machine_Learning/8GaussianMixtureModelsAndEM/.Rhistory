I=(resultat$statistic)/n
I
sum(res.afc$row$inertia)
sum(res.afc$col$inertia)
sum(lambda)
N=universite [,1:6]
N
F=N/sum(N) #matrice des fréquences
F
N=universite[1:10,1:6]
F <- N/sum(N) # matrice des fréquences relatives
F
resultat=chisq.test(M) # dépendance largement significative
I=(resultat$statistic)/1034360 # n=effectif total = 1034360 étudiants
I
resultat = chisq.test(N)
resultat #dépendance largement significative
summary (resultat)
n = sum(N)
n
I=(resultat$statistic)/n
I
#3.	Créer la matrice des résidus standardisés
standard.resid=((resultat$observed)-(resultat$expected))/(sqrt(resultat$expected))
standard.resid
D=1/sqrt(1034360)*standard.resid # matrice à diagonaliser
#3.	Créer la matrice des résidus standardisés
standard.resid=((resultat$observed)-(resultat$expected))/(sqrt(resultat$expected))
standard.resid
D=1/sqrt(1034360)*standard.resid # matrice à diagonaliser
D
#3.	Créer la matrice des résidus standardisés
standard.resid=((resultat$observed)-(resultat$expected))/(sqrt(resultat$expected))
standard.resid
D=1/sqrt(1034360)*standard.resid # matrice à diagonaliser
D
#3
S=((resultat$observed)-(resultat$expected))/sqrt(resultat$expected)
S
sum(S^2)
resultat$statistic
M=1/sqrt(n)*S #matrice à diagonaliser
M #p. 58 du cours
#4.décomposition en valeurs singulières de D (vérifier que sigma²=lambda)
decomp=svd(D)
decomp
#5. coordonnées profils-lignes =1/racine(fi.)*U*A
r <- apply(F,1,sum) # vecteur profil-ligne moyen f.j
c <- apply(F,2,sum) # vecteur profil-colonne moyen fi.
R<- diag(1/sqrt(r))
C<-diag(1/sqrt(c))
A=R%*%decomp$u%*%diag(decomp$d) # matrice des coordonnée des profils-lignes
A
res.afc$row$coord
#6.coordonnées profils-colonnes = 1/racine(f.j)*V*A
B=C%*%decomp$v%*%diag(decomp$d)
B
res.afc$col$coord
# 7. superposition des deux nuages de points
plot(A[,1:2],xlab="dim 1",ylab="dim 2",xlim=c(-0.6,0.6),ylim=c(-0.6,0.6),main="Premier plan factoriel")
abline(v = 0, lty = 2)
abline(h = 0, lty = 2)
text(A[,1:2],rownames(N),pos=4)
points(B[,1:2],pch=2,col=2)
text(B[,1:2],colnames(N),pos=4,col=2)
fviz_ca_biplot(res.afc,repel = TRUE)
#Q.1
library(MASS)
#Q2
training=Boston[1:400,]
#dim(train)
testing=Boston[-(1:400),]
#Q3
cor(Boston$age,Boston$medv)
#Q4
model=lm(medv~age,data=training)
#medv = median house value
plot(training$age,training$medv,col='green',xlab="Age",ylab="MEDV= Median House Value",pch=20)
abline(model,col='red',lwd=3)
#Q5
modelBis=lm(medv~log(lstat)+age,data=training)
modelBis
abline(modelBis)
#Q6
summary(modelBis)
#Q8
y_hat=predict(model,data.frame(lstat=(testing$lstat),age=(testing$age)))
y=testing$medv
RMSE=mean((y-y_hat)^2)
#Q9
modelThird= lm(medv~.,data=training)
modelThird
summary(modelThird)
#Q10
modelFourth=lm(medv~.+log(lstat)-lstat,data=training)
modelFourth
summary(modelFourth)
#Q12
round(cor(training),digits=2)
#Q13
library(corrplot)
?corrplot.mixed
M<-cor(Boston)
corrplot.mixed(M,tl.col="Black")
#Q14
cor(Boston$rad,Boston$tax)
#Q15
modelFifth=lm(medv~.+log(lstat)-lstat-tax,data=training)
summary(modelFifth)
summary(modelFourth)
#Q16
y_hatModelFifth=predict(modelFifth,testing)
y_hatModelFifth
RMSE=mean((testing$medv-y_hatModelFifth)^2)
RMSE
#ANOVA
#Q17
length(which(Boston$chas==1))
#Q18
boxplot(Boston$medv~Boston$chas)
#Q19
aggregate(training$medv,by=list(training$chas),FUN=mean)
#20
#20
#20
#Q19
aggregate(training$medv,by=list(training$chas),FUN=mean)
#20
?aov()
modelsixth=aov(medv~chas,data=training)
modelsixth
typeof(modelsixth)
modelsixth
anova(modelsixth)
summary(modelsixth)
modelsixth=aov(medv~chas,data=training)
anova(modelsixth)
summary(modelsixth)
h
modelsixth=aov(medv~chas,data=training)
anova(modelsixth)
#Q8
y_hat=predict(model,data.frame(lstat=(testing$lstat),age=(testing$age)))
y=testing$medv
RMSE=mean((y-y_hat)^2)
#Q9
modelThird= lm(medv~.,data=training)
modelThird
summary(modelThird)
modelsixthBis= lm(medv~chas.,data=training)
modelsixthBis
modelsixth=aov(medv~chas,data=training)
anova(modelsixth)
modelsixthBis= lm(medv~chas.,data=training)
modelsixthBis
modelsixthBis= lm(medv~chas,data=training)
modelsixthBis
#20
an = aov(medv ~ chas, data=train)
anova(an)
#2
train = Boston[1:400,]
#20
an = aov(medv ~ chas, data=train)
anova(an)
anova(modelsixth)
modelsixthBis
summary(modelsixthBis)
modelsixth=aov(medv~chas,data=training)
anova(modelsixth)
#Q5
modelBis=lm(medv~log(lstat)+age,data=training)
modelBis
abline(modelBis)
#Q.1
library(MASS)
#Q2
training=Boston[1:400,]
#dim(train)
testing=Boston[-(1:400),]
#Q3
cor(Boston$age,Boston$medv)
#Q4
model=lm(medv~age,data=training)
#medv = median house value
plot(training$age,training$medv,col='green',xlab="Age",ylab="MEDV= Median House Value",pch=20)
abline(model,col='red',lwd=3)
#Q5
modelBis=lm(medv~log(lstat)+age,data=training)
modelBis
abline(modelBis)
#Q6
summary(modelBis)
abline(modelBis)
modelBis
anova(modelsixth)
#Q16
y_hatModelFifth=predict(modelFifth,testing)
y_hatModelFifth
RMSE=mean((testing$medv-y_hatModelFifth)^2)
RMSE
#ANOVA
#Q17
length(which(Boston$chas==1))
#Q18
boxplot(Boston$medv~Boston$chas)
#Q19
aggregate(training$medv,by=list(training$chas),FUN=mean)
#20
?aov()
modelsixth=aov(medv~chas,data=training)
anova(modelsixth)
modelsixthBis= lm(medv~chas,data=training)
summary(modelsixthBis)
#We find the same result for the p value which is equal to 0.005 (0.5%) so lower than 1%
#We find the same result for the p value which is equal to 0.005 (0.5%) so lower than 1%
#We print summary modelSixth Bis in order to verify our result
anova(modelsixth)
summary(modelsixthBis)
#Q.1
library(MASS)
#Q2
training=Boston[1:400,]
#dim(train)
testing=Boston[-(1:400),]
#Q3
cor(Boston$age,Boston$medv)
#Q4
model=lm(medv~age,data=training)
#medv = median house value
plot(training$age,training$medv,col='green',xlab="Age",ylab="MEDV= Median House Value",pch=20)
abline(model,col='red',lwd=3)
#Q5
modelBis=lm(medv~log(lstat)+age,data=training)
modelBis
abline(modelBis)
#Q5
modelBis=lm(medv~log(lstat)+age,data=training)
modelBis
abline(modelBis)
#Q8
y_hat=predict(modelBis,data.frame(lstat=(testing$lstat),age=(testing$age)))
y=testing$medv
#Q.1
library(MASS)
#Q2
training=Boston[1:400,]
#dim(train)
testing=Boston[-(1:400),]
#Q3
cor(Boston$age,Boston$medv)
#Q4
model=lm(medv~age,data=training)
#medv = median house value
plot(training$age,training$medv,col='green',xlab="Age",ylab="MEDV= Median House Value",pch=20)
abline(model,col='red',lwd=3)
#Q5
modelBis=lm(medv~log(lstat)+age,data=training)
modelBis
abline(modelBis)
#Q6
summary(modelBis)
#Q8
y_hat=predict(modelBis,data.frame(lstat=(testing$lstat),age=(testing$age)))
y=testing$medv
RMSE=mean((y-y_hat)^2)
#Q9
modelThird= lm(medv~.,data=training)
RMSE
#Q8
y_hat=predict(model,data.frame(lstat=(testing$lstat),age=(testing$age)))
y=testing$medv
RMSE=mean((y-y_hat)^2)
RMSE
#Q8
y_hat=predict(modelBis,data.frame(lstat=(testing$lstat),age=(testing$age)))
y=testing$medv
RMSE=mean((y-y_hat)^2)
RMSE
#Q6
summary(modelBis)
summary(model)
#Q8
y_hat=predict(model,data.frame(lstat=(testing$lstat),age=(testing$age)))
y=testing$medv
RMSE=mean((y-y_hat)^2)
RMSE
#Q8
y_hat=predict(modelBis,data.frame(lstat=(testing$lstat),age=(testing$age)))
y=testing$medv
RMSE=mean((y-y_hat)^2)
RMSE
#Q6
summary(modelBis)
#Q9
modelThird= lm(medv~.,data=training)
modelThird
summary(modelThird)
#Q10
modelFourth=lm(medv~.+log(lstat)-lstat,data=training)
modelFourth
summary(modelFourth)
#Q12
round(cor(training),digits=2)
#Q13
library(corrplot)
?corrplot.mixed
M<-cor(Boston)
corrplot.mixed(M,tl.col="Black")
#Q14
cor(Boston$rad,Boston$tax)
#Q15
modelFifth=lm(medv~.+log(lstat)-lstat-tax,data=training)
summary(modelFifth)
summary(modelFourth)
#Q16
y_hatModelFifth=predict(modelFifth,testing)
y_hatModelFifth
RMSE=mean((testing$medv-y_hatModelFifth)^2)
RMSE
#ANOVA
#Q17
length(which(Boston$chas==1))
#Q18
boxplot(Boston$medv~Boston$chas)
#Q19
aggregate(training$medv,by=list(training$chas),FUN=mean)
#20
modelsixth=aov(medv~chas,data=training)
anova(modelsixth)
modelsixthBis= lm(medv~chas,data=training)
summary(modelsixthBis)
#We find the same result for the p value which is equal to 0.005 (0.5%) so lower than 1%
#We find the same result for the p value which is equal to 0.005 (0.5%) so lower than 1%
#We print summary modelSixth Bis in order to verify our result
#21
#21
#21
?Boston
#Q6
summary(modelBis)
#Q18
boxplot(Boston$medv~Boston$chas)
#21
ano=aov(medv~crim+chas,data=training)
ano
#21
ano=aov(medv~chas+crim,data=training)
ano
ano$coefficients
ano$rank
ano$effects
ano$fitted.values
ano$assign
ano$call
ano$coefficients
summary(ano)
#23
modelseventh=lm(medv~lstat*age,data=training)
modelseventh
#23
modelseventh=lm(formula=(medv~lstat*age),data=training)
modelseventh
summary(modelseventh)
#24
modelFinal=lm(medv~.*.,data=training)
summary(modelseventh)
#24
modelFinal=lm(medv~.:.,data=training)
summary(modelseventh)
#24
modelFinal=lm(medv~.*.,data=training)
summary(modelseventh)
#24
modelFinal=lm(medv~.*.,data=training)
summary(modelFinal)
#this one tries every combination of features
#we can see the fiability thanks to the stars next to p value
#so rad:tax, rm:lstat,rm:black,chas:black give the best p values
?Boston
#dim(train)
testing=Boston[-(1:400),]
#Q3
cor(Boston$age,Boston$medv)
#Q4
model=lm(medv~age,data=training)
#medv = median house value
plot(training$age,training$medv,col='green',xlab="Age",ylab="MEDV= Median House Value",pch=20)
abline(model,col='red',lwd=3)
#Q5
modelBis=lm(medv~log(lstat)+age,data=training)
modelBis
abline(modelBis)
#Q6
summary(modelBis)
#Q8
y_hat=predict(modelBis,data.frame(lstat=(testing$lstat),age=(testing$age)))
y=testing$medv
RMSE=mean((y-y_hat)^2)
RMSE
#Q9
modelThird= lm(medv~.,data=training)
modelThird
summary(modelThird)
#Q.1
library(MASS)
#Q2
training=Boston[1:400,]
#dim(train)
testing=Boston[-(1:400),]
#Q3
cor(Boston$age,Boston$medv)
#Q4
model=lm(medv~age,data=training)
#medv = median house value
plot(training$age,training$medv,col='green',xlab="Age",ylab="MEDV= Median House Value",pch=20)
abline(model,col='red',lwd=3)
#Q5
modelBis=lm(medv~log(lstat)+age,data=training)
modelBis
abline(modelBis)
#Q6
summary(modelBis)
#Q8
y_hat=predict(modelBis,data.frame(lstat=(testing$lstat),age=(testing$age)))
y=testing$medv
RMSE=mean((y-y_hat)^2)
RMSE
#Q9
modelThird= lm(medv~.,data=training)
modelThird
summary(modelThird)
#Q10
modelFourth=lm(medv~.+log(lstat)-lstat,data=training)
modelFourth
summary(modelFourth)
#Q12
round(cor(training),digits=2)
#Q13
library(corrplot)
?corrplot.mixed
M<-cor(Boston)
corrplot.mixed(M,tl.col="Black")
#Q14
cor(Boston$rad,Boston$tax)
#Q15
modelFifth=lm(medv~.+log(lstat)-lstat-tax,data=training)
summary(modelFifth)
summary(modelFourth)
#Q16
y_hatModelFifth=predict(modelFifth,testing)
y_hatModelFifth
RMSE=mean((testing$medv-y_hatModelFifth)^2)
RMSE
#ANOVA
#Q17
length(which(Boston$chas==1))
#Q18
boxplot(Boston$medv~Boston$chas)
#Q19
aggregate(training$medv,by=list(training$chas),FUN=mean)
#20
modelsixth=aov(medv~chas,data=training)
anova(modelsixth)
modelsixthBis= lm(medv~chas,data=training)
summary(modelsixthBis)
#21
ano=aov(medv~chas+crim,data=training)
ano$coefficients
summary(ano)
#23
modelseventh=lm(medv~lstat*age,data=training)
summary(modelseventh)
#24
modelFinal=lm(medv~.*.,data=training)
summary(modelFinal)
#this one tries every combination of features
#this one tries every combination of features
#we can see the fiability thanks to the stars next to p value
#this one tries every combination of features
#we can see the fiability thanks to the stars next to p value
#so rad:tax, rm:lstat,rm:black,chas:black give the best p values
#this one tries every combination of features
#we can see the fiability thanks to the stars next to p value
#so rad:tax, rm:lstat,rm:black,chas:black give the best p values
#this one tries every combination of features
#we can see the fiability thanks to the stars next to p value
#so rad:tax, rm:lstat,rm:black,chas:black give the best p values
#this one tries every combination of features
#we can see the fiability thanks to the stars next to p value
#so rad:tax, rm:lstat,rm:black,chas:black give the best p values
#Q3
cor(Boston$age,Boston$medv)
#Q4
model=lm(medv~age,data=training)
#medv = median house value
plot(training$age,training$medv,col='green',xlab="Age",ylab="MEDV= Median House Value",pch=20)
abline(model,col='red',lwd=3)
#Q5
modelBis=lm(medv~log(lstat)+age,data=training)
par(mfrow=c(2,2))
setwd("C:/Users/TRANG THOMAS/.spyder-py3/Desktop/ESILV/A4/S7/Machine_Learning/8GaussianMixtureModelsAndEM")
data1<-read.csv('data1.csv')
data2<-read.csv('data2.csv')
library(ggpubr)
gg1<-ggplot(data1, aes(X1,X2,color=as.factor(truth)))+geom_point()+scale_color_manual(values=c("#000000", "#FF0000", "#009900","#000099"))+ggtitle('Data 1')
gg2<-ggplot(data2, aes(X1,X2,color=as.factor(truth)), binwidth = 1)+geom_point()+scale_color_manual(values=c("#000000", "#FF0000", "#009900","#000099"))+ggtitle('Data 2')
ggarrange(gg1,gg2)
print('a')
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
print('a')
