ano$fitted.values
ano$assign
ano$call
ano$coefficients
summary(ano)
#23
modelseventh=lm(medv~lstat*age,data=training)
modelseventh
#23
modelseventh=lm(formula=(medv~lstat*age),data=training)
modelseventh
summary(modelseventh)
#24
modelFinal=lm(medv~.*.,data=training)
summary(modelseventh)
#24
modelFinal=lm(medv~.:.,data=training)
summary(modelseventh)
#24
modelFinal=lm(medv~.*.,data=training)
summary(modelseventh)
#24
modelFinal=lm(medv~.*.,data=training)
summary(modelFinal)
#this one tries every combination of features
#we can see the fiability thanks to the stars next to p value
#so rad:tax, rm:lstat,rm:black,chas:black give the best p values
?Boston
#dim(train)
testing=Boston[-(1:400),]
#Q3
cor(Boston$age,Boston$medv)
#Q4
model=lm(medv~age,data=training)
#medv = median house value
plot(training$age,training$medv,col='green',xlab="Age",ylab="MEDV= Median House Value",pch=20)
abline(model,col='red',lwd=3)
#Q5
modelBis=lm(medv~log(lstat)+age,data=training)
modelBis
abline(modelBis)
#Q6
summary(modelBis)
#Q8
y_hat=predict(modelBis,data.frame(lstat=(testing$lstat),age=(testing$age)))
y=testing$medv
RMSE=mean((y-y_hat)^2)
RMSE
#Q9
modelThird= lm(medv~.,data=training)
modelThird
summary(modelThird)
#Q.1
library(MASS)
#Q2
training=Boston[1:400,]
#dim(train)
testing=Boston[-(1:400),]
#Q3
cor(Boston$age,Boston$medv)
#Q4
model=lm(medv~age,data=training)
#medv = median house value
plot(training$age,training$medv,col='green',xlab="Age",ylab="MEDV= Median House Value",pch=20)
abline(model,col='red',lwd=3)
#Q5
modelBis=lm(medv~log(lstat)+age,data=training)
modelBis
abline(modelBis)
#Q6
summary(modelBis)
#Q8
y_hat=predict(modelBis,data.frame(lstat=(testing$lstat),age=(testing$age)))
y=testing$medv
RMSE=mean((y-y_hat)^2)
RMSE
#Q9
modelThird= lm(medv~.,data=training)
modelThird
summary(modelThird)
#Q10
modelFourth=lm(medv~.+log(lstat)-lstat,data=training)
modelFourth
summary(modelFourth)
#Q12
round(cor(training),digits=2)
#Q13
library(corrplot)
?corrplot.mixed
M<-cor(Boston)
corrplot.mixed(M,tl.col="Black")
#Q14
cor(Boston$rad,Boston$tax)
#Q15
modelFifth=lm(medv~.+log(lstat)-lstat-tax,data=training)
summary(modelFifth)
summary(modelFourth)
#Q16
y_hatModelFifth=predict(modelFifth,testing)
y_hatModelFifth
RMSE=mean((testing$medv-y_hatModelFifth)^2)
RMSE
#ANOVA
#Q17
length(which(Boston$chas==1))
#Q18
boxplot(Boston$medv~Boston$chas)
#Q19
aggregate(training$medv,by=list(training$chas),FUN=mean)
#20
modelsixth=aov(medv~chas,data=training)
anova(modelsixth)
modelsixthBis= lm(medv~chas,data=training)
summary(modelsixthBis)
#21
ano=aov(medv~chas+crim,data=training)
ano$coefficients
summary(ano)
#23
modelseventh=lm(medv~lstat*age,data=training)
summary(modelseventh)
#24
modelFinal=lm(medv~.*.,data=training)
summary(modelFinal)
#this one tries every combination of features
#this one tries every combination of features
#we can see the fiability thanks to the stars next to p value
#this one tries every combination of features
#we can see the fiability thanks to the stars next to p value
#so rad:tax, rm:lstat,rm:black,chas:black give the best p values
#this one tries every combination of features
#we can see the fiability thanks to the stars next to p value
#so rad:tax, rm:lstat,rm:black,chas:black give the best p values
#this one tries every combination of features
#we can see the fiability thanks to the stars next to p value
#so rad:tax, rm:lstat,rm:black,chas:black give the best p values
#this one tries every combination of features
#we can see the fiability thanks to the stars next to p value
#so rad:tax, rm:lstat,rm:black,chas:black give the best p values
#Q3
cor(Boston$age,Boston$medv)
#Q4
model=lm(medv~age,data=training)
#medv = median house value
plot(training$age,training$medv,col='green',xlab="Age",ylab="MEDV= Median House Value",pch=20)
abline(model,col='red',lwd=3)
#Q5
modelBis=lm(medv~log(lstat)+age,data=training)
setwd("C:/Users/TRANG THOMAS/.spyder-py3/Desktop/ESILV/A4/S7/Machine Learning/PW6")
#1.
irisdf<-read.csv('iris.data')
names(irisdf)
#Exploratory analysis
#2.
#compute 4 boxplots in the same figure
par(mfrow=c(2,2))
plot1<-boxplot(sepal_length~class,data=irisdf,col=c("green","yellow","orange"))
plot2<-boxplot(sepal_width~class,data=irisdf,col=c("green","yellow","orange"))
plot3<-boxplot(petal_length~class,data=irisdf,col=c("green","yellow","orange"))
plot4<-boxplot(petal_width~class,data=irisdf,col=c("green","yellow","orange"))
#The SETOSA has the lowest means for: sepal and petal length, petal width with small interquartile range
#but the highest mean sepal width with a large interquartile range
#The VERSICOLOR has the 2nd means for: sepal and petal length, petal width with small interquartile range
#but the lowest mean for sepal width with a large interquartile range
#The VIRGINCA has the highest means for: sepal and petal length, petal width with large interquartile range
#and the 2nd mean for sepal width with a medium interquartile range
#3.
# Let's use the ggplot2 library
# ggplot2 is the most advanced package for data visualization
# gg corresponds to The Grammar of Graphics.
library(ggplot2)
library("ggpubr")
#of course you must install it first if you don't have it already
# histogram of sepal_length
plot1gg<-ggplot(irisdf, aes(x=sepal_length, fill=class)) +
geom_histogram(binwidth=.2, alpha=.5)
# histogram of sepal_width
plot2gg<-ggplot(irisdf, aes(x=sepal_width, fill=class)) +
geom_histogram(binwidth=.2, alpha=.5)
# histogram of petal_length
plot3gg<-ggplot(irisdf, aes(x=petal_length, fill=class)) +
geom_histogram(binwidth=.2, alpha=.5)
# histogram of petal_width
plot4gg<-ggplot(irisdf, aes(x=petal_width, fill=class)) +
geom_histogram(binwidth=.2, alpha=.5)
ggarrange(plot1gg,plot2gg,plot3gg,plot4gg,ncol=2,nrow=2)
#PCA using princomp()
#4.
pcairis=princomp(iris[,-5], cor=T)
str(pcairis)
summary(pcairis)
plot(pcairis)
#PC1 contains a lot of information
#PC1 and PC2 contain 96% of the information
#PC3 and PC4 contains only 4% of the information
#So we should only keep the 2 first variables
biplot(pcairis)
#sepal width is more impotant for PC2 because it is closer to the vetical axis
#Sepal/petal lengh and Petal width are more important for PC1 beacause they are closer to
#Deeper PCA using factoextra package
#5.
library(factoextra)
#ScreePlot
fviz_eig(pcairis, addlabels = TRUE)
#we can see that we have alreadu 96% of information (73% for the first one and 23% for the second) on the 2 first components which is already high
#the last component is not that important thats why we could choose to not consider it
#Graph of individuals
fviz_pca_ind(pcairis, col.ind = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE
)
#We choose to classify our individuals by color in function of the cos2
#All individuals which are similar are grouped on the graph
#we can see many clusters of individuals
#Graph of variables
fviz_pca_var(pcairis, col.var="contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE
)
#we can see that the variable petal length contributes the most to dimension 1 and
#sepal width contributes the most to dimensihon 2
#Petal length and petal width contribute a lot to dimension 1
fviz_pca_biplot(pcairis, repel = TRUE)
#There are too many indivduals so the graph is illisible
#individuals on the group on the left give a high value to the variable "Sepal Width"
#individuals on the group on the right give a high value to the 3 others variables
#contribution of varibles to components 1 & 2
# Contributions of variables to PC1
cont1<-fviz_contrib(pcairis, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
cont2<-fviz_contrib(pcairis, choice = "var", axes = 2, top = 10)
ggarrange(cont1,cont2,ncol=2,nrow=1)
#We can see that for the component 1, variables petal and sepal length and petal witdh contributes a lot
#for the component 2 sepal width contributes the most as we said earlier
#Step by step PCA
#6
#X represents data
#y represents the labels
X <- iris[,-5]
y <- iris[,5]
#7. standardizing
X_scaled<-scale(X,center=TRUE,scale=TRUE)
#8. Covariance Matrix
#2 diferent methods to get the cov matrix
covMat<-cov(X_scaled)
covMat
#calcul method
# %*% :do a matrix multiplication
M=colMeans(X_scaled)
covMatr<-(t(X_scaled-M)%*%(X_scaled-M))/(nrow(X_scaled)-1)
covMatr
#9 Eigen values and EigenVectors
eigen(covMatr)
#We obtain Eigenvalues and eigen vectors
#Correlation Matrix
#10.
corrMat<-cor(X)
eigen(corrMat)
# the same method except the fact that we do with correlation mat
# we have same results
#11
corrMatScaled<-cor(X_scaled)
eigen(corrMatScaled)
#We have same results as previous 3 questions with same eigenvalues and eigen vectors
#Explained variance
#12
temp<-eigen(cor(X))$values
sommeVals<-sum(temp)
pc1<-c(temp[1]/sommeVals,sum(temp[1]/sommeVals))
pc2<-c(temp[2]/sommeVals,sum(temp[1:2]/sommeVals))
pc3<-c(temp[3]/sommeVals,sum(temp[1:3]/sommeVals))
pc4<-c(temp[4]/sommeVals,sum(temp[1:4]/sommeVals))
res<-data.frame(pc1,pc2,pc3,pc4)
#the first line each of the eigen vals
#the second line is the cumulative results of eigen vals
#13
res1<-data.frame(pc=c('pc1','pc2','pc3','pc4'),
val=c(res[1,1],res[1,2],res[1,3],res[1,4]))
res1
ggplot(data=res1,aes(x=pc,y=val))+geom_bar(stat='identity')
#we find the same distribution pc as in the beginning as 73% for the first, 23% for the second
#projection matrix
#14
projeMat<-eigen(cor(X))$vectors[,1:2]
#we take 2 first columns of eigenvectors as it was asked
#15
yres<-X_scaled%*%projeMat
#Y=X*A with A=projeMat
#16
dffinal<-data.frame(yres)
names(dffinal)<-c('pc1','pc2')
dffinal$class<-irisdf$class
#we put classes on final df
ggplot(data=dffinal,aes(x=pc1,y=pc2,color=class))+geom_point()
#We color in function of the class
#we can see 3 different classes which are well separated
mtcars<-
cars
mtcars<-
cars
mtcars<-
mtcars
mtcars<-
data(mtcars)
data(mtcars)
mtcars
names(mtcars)
boxplot(mpg~carb,data=mtcars,col=c("green","yellow","orange"))
plot5<-boxplot(mpg~carb,data=mtcars,col=c("green","yellow","orange"))
plot5<-boxplot(mpg~carb,data=mtcars,col=c("green","yellow","orange"))
boxplot(mpg~carb,data=mtcars,col=c("green","yellow","orange"))
boxplot(mpg~carb,data=mtcars,col=c("green","yellow","orange"))
boxplot(cyl~carb,data=mtcars,col=c("green","yellow","orange"))
boxplot(disp~carb,data=mtcars,col=c("green","yellow","orange"))
boxplot(hp~carb,data=mtcars,col=c("green","yellow","orange"))
ggplot(mtcars, aes(x=mpg, fill=class)) +
geom_histogram(binwidth=.2, alpha=.5)
ggplot(mtcars, aes(x=mpg, fill=mpg)) +
geom_histogram(binwidth=.2, alpha=.5)
pcaCars=princomp(mtcars[,],cor=T)
spcaCars)
pca(mtcars)
summary(pcaCars)
plot(pcaCars)
biplot(pcaCars)
plot(pcaCars)
#we can see that the cp1 and cp2 is much more important than the 7 others
#they gather a lot of information
biplot(pcaCars)
#we can see the individuals and the features which are very important for them
fviz_eig(pcaCars,addlabels = TRUE)
#we can see that the cp1 and cp2 and cp3 is much more important they gather 90% of information
#whereas the 6 others gather 10%
fviz_pca_ind(pcaCars, col.ind = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE
)
fviz_pca_var(pcaCars, col.var="contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE
)
fviz_pca_biplot(pcaCars, repel = TRUE)
cont1<-fviz_contrib(pcairis, choice = "var", axes = 1, top = 10)
cont3<-fviz_contrib(pcaCars, choice = "var", axes = 1, top = 10)
cont4<-fviz_contrib(pcaCars, choice = "var", axes = 2, top = 10)
ggarrange(cont1,cont2,ncol=2,nrow=1)
ggarrange(cont3,cont4,ncol=2,nrow=1)
X
y
#PCA STEP BY STEP MT CARS
mtCarsScaled<-scale(mtcars,center=TRUE,scale=TRUE)
mtCarsScaled
covMatCars<-cov(mtCarsScaled)
covMatCars
boxplot(mpg~carb,data=mtcars,col=c("green","yellow","orange"))
#we can see a linear relation between mpg and carb, indeed for a high MPG we have a low CARB
boxplot(cyl~carb,data=mtcars,col=c("green","yellow","orange"))
#Here for a high CARB we have a high CYL
boxplot(disp~carb,data=mtcars,col=c("green","yellow","orange"))
#also linear relation for a high carb we have a high DISP
boxplot(hp~carb,data=mtcars,col=c("green","yellow","orange"))
#also linear relation we have a high HP for a High CARB
ggplot(mtcars, aes(x=mpg, fill=mpg)) +
geom_histogram(binwidth=.2, alpha=.5)
#The majority of cars are in middle value of MPG
pcaCars=princomp(mtcars[,],cor=T)
summary(pcaCars)
biplot(pcaCars)
#we can see that the cp1 and cp2 and cp3 is much more important they gather 90% of information
#whereas the 6 others gather 10%
fviz_pca_ind(pcaCars, col.ind = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE
)
fviz_pca_var(pcaCars, col.var="contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE
)
# for dimension 1 the most important features are MPG CYL and disp
#for dimension 2 the most important features are GSEC,GEAR
fviz_pca_biplot(pcaCars, repel = TRUE)
cont3<-fviz_contrib(pcaCars, choice = "var", axes = 1, top = 10)
cont4<-fviz_contrib(pcaCars, choice = "var", axes = 2, top = 10)
ggarrange(cont3,cont4,ncol=2,nrow=1)
setwd("C:/Users/TRANG THOMAS/.spyder-py3/Desktop/ESILV/A4/S7/Machine_Learning/7Kmeans")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
ligue1 <- read.csv("ligue1_17_18.csv", row.names=1, sep=";")
# Chunk 3
require(knitr)
kable(ligue1[0:2,])
dim(ligue1)[2]
# Chunk 4
pointCards=data.frame(ligue1$Points,ligue1$yellow.cards)
pointCards
# Chunk 5
set.seed(1234)
km <- kmeans(pointCards,centers=2,iter.max = 20)
# Chunk 6
km
# Chunk 7
km$centers
# Chunk 8
plot(pointCards$ligue1.Points,pointCards$ligue1.yellow.cards,col=km$cluster,pch=19,cex=1.2)
points(km$centers,pch=4,cex=3,lwd=3,col=c('black','red'))
text(pointCards$ligue1.Points,pointCards$ligue1.yellow.cards,labels=row.names(ligue1),col=km$cluster)
# Chunk 9
km3 <- kmeans(pointCards,centers=3,iter.max = 20)
km4 <- kmeans(pointCards,centers=4,iter.max = 20)
km5 <- kmeans(pointCards,centers=5,iter.max = 20)
par(mfrow=c(2,1),cex=0.6)
par(fig=c(0,1,0.5,1))
plot(pointCards$ligue1.Points,pointCards$ligue1.yellow.cards,col=km3$cluster,pch=19,cex=1.2,main='3 clusters')
points(km3$centers,pch=4,cex=3,lwd=3,col=c('black','red','green'))
text(pointCards$ligue1.Points,pointCards$ligue1.yellow.cards,labels=row.names(ligue1),col=km3$cluster)
par(fig=c(0,1,0,0.5), new=TRUE)
plot(pointCards$ligue1.Points,pointCards$ligue1.yellow.cards,col=km4$cluster,pch=19,cex=1.2,main='4 clusters')
points(km4$centers,pch=4,cex=3,lwd=3,col=c('black','red','green','blue'))
text(pointCards$ligue1.Points,pointCards$ligue1.yellow.cards,labels=row.names(ligue1),col=km4$cluster)
# Chunk 10
wss <- (nrow(ligue1)-1)*sum(apply(ligue1,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(ligue1,
centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
# Chunk 11
round(km3$betweenss/km3$totss,3)
round(km4$betweenss/km4$totss,3)
wss <- (nrow(ligue1)-1)*sum(apply(ligue1,2,var))
for (i in 2:15) wss[i] <- (kmeans(ligue1,centers=i)$betweenss)/(kmeans(ligue1,
centers=i)$totss)
plot(2:15, wss[-1], type="b", xlab="Number of Clusters",
ylab="Between sum of squares divide by total sum of squares")
# Chunk 12
ligue1_scaled<-data.frame(scale(ligue1,center=TRUE,scale=TRUE))
# Chunk 13
set.seed(1234)
km.ligue1 <- kmeans(ligue1,centers=3,iter.max = 20)
km.ligue1_scaled <- kmeans(ligue1_scaled,centers=3,iter.max = 20)
# Chunk 14
table(km.ligue1$cluster)
table(km.ligue1_scaled$cluster)
set.seed(1234)
pcaligue1=princomp(ligue1[,],cor=T)
summary(pcaligue1)
par(mfrow=c(1,1))
biplot(pcaligue1,panel.firsy=grid())
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
ligue1 <- read.csv("ligue1_17_18.csv", row.names=1, sep=";")
# Chunk 3
require(knitr)
kable(ligue1[0:2,])
dim(ligue1)[2]
# Chunk 4
pointCards=data.frame(ligue1$Points,ligue1$yellow.cards)
pointCards
# Chunk 5
set.seed(1234)
km <- kmeans(pointCards,centers=2,iter.max = 20)
# Chunk 6
km
# Chunk 7
km$centers
# Chunk 8
plot(pointCards$ligue1.Points,pointCards$ligue1.yellow.cards,col=km$cluster,pch=19,cex=1.2)
points(km$centers,pch=4,cex=3,lwd=3,col=c('black','red'))
text(pointCards$ligue1.Points,pointCards$ligue1.yellow.cards,labels=row.names(ligue1),col=km$cluster)
# Chunk 9
km3 <- kmeans(pointCards,centers=3,iter.max = 20)
km4 <- kmeans(pointCards,centers=4,iter.max = 20)
km5 <- kmeans(pointCards,centers=5,iter.max = 20)
par(mfrow=c(2,1),cex=0.6)
par(fig=c(0,1,0.5,1))
plot(pointCards$ligue1.Points,pointCards$ligue1.yellow.cards,col=km3$cluster,pch=19,cex=1.2,main='3 clusters')
points(km3$centers,pch=4,cex=3,lwd=3,col=c('black','red','green'))
text(pointCards$ligue1.Points,pointCards$ligue1.yellow.cards,labels=row.names(ligue1),col=km3$cluster)
par(fig=c(0,1,0,0.5), new=TRUE)
plot(pointCards$ligue1.Points,pointCards$ligue1.yellow.cards,col=km4$cluster,pch=19,cex=1.2,main='4 clusters')
points(km4$centers,pch=4,cex=3,lwd=3,col=c('black','red','green','blue'))
text(pointCards$ligue1.Points,pointCards$ligue1.yellow.cards,labels=row.names(ligue1),col=km4$cluster)
# Chunk 10
wss <- (nrow(ligue1)-1)*sum(apply(ligue1,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(ligue1,
centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
# Chunk 11
round(km3$betweenss/km3$totss,3)
round(km4$betweenss/km4$totss,3)
wss <- (nrow(ligue1)-1)*sum(apply(ligue1,2,var))
for (i in 2:15) wss[i] <- (kmeans(ligue1,centers=i)$betweenss)/(kmeans(ligue1,
centers=i)$totss)
plot(2:15, wss[-1], type="b", xlab="Number of Clusters",
ylab="Between sum of squares divide by total sum of squares")
# Chunk 12
ligue1_scaled<-data.frame(scale(ligue1,center=TRUE,scale=TRUE))
# Chunk 13
set.seed(1234)
km.ligue1 <- kmeans(ligue1,centers=3,iter.max = 20)
km.ligue1_scaled <- kmeans(ligue1_scaled,centers=3,iter.max = 20)
# Chunk 14
table(km.ligue1$cluster)
table(km.ligue1_scaled$cluster)
# Chunk 15
set.seed(1234)
pcaligue1=princomp(ligue1[,],cor=T)
summary(pcaligue1)
par(mfrow=c(1,1))
biplot(pcaligue1,panel.firsy=grid())
